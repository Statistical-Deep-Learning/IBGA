{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17241c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "import  numpy as np\n",
    "from    data import load_data, preprocess_features, preprocess_adj, load_data_all\n",
    "from    utils import masked_loss, masked_acc, sparse_dropout\n",
    "import warnings\n",
    "# import faiss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda34c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='cora', model='gcn', learning_rate=0.03, epochs=2000, hidden=32, weight_decay=0.0005, dropout=0.0)\n"
     ]
    }
   ],
   "source": [
    "import  argparse\n",
    "args = argparse.ArgumentParser()\n",
    "args.add_argument('--dataset', default='cora')\n",
    "args.add_argument('--model', default='gcn')\n",
    "args.add_argument('--learning_rate', type=float, default=0.03)\n",
    "args.add_argument('--epochs', type=int, default=2000)\n",
    "args.add_argument('--hidden', type=int, default=32)\n",
    "args.add_argument('--weight_decay', type=float, default=5e-4)\n",
    "args.add_argument('--dropout', type=float, default=0.0)\n",
    "args = args.parse_args(['--dataset', 'cora'])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023efb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.stats import mode\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def run_kmeans(x, y, num_cluster, niter=100):\n",
    "    print('performing kmeans clustering')\n",
    "    \n",
    "    # Run kmeans to find centroids\n",
    "    # centroids, distortion = kmeans(x, num_cluster, iter=niter)\n",
    "    \n",
    "    # # Assign samples to the nearest centroids\n",
    "    # cluster_assignments, _ = vq(x, centroids)\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0)\n",
    "    cluster_assignments = kmeans.fit_predict(x)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Map each cluster to the most frequent class label and reorder centroids\n",
    "    # 这里如果两个cluster的mode of lable 要是一致的话，后面的centroids会替换掉前面的\n",
    "    reordered_centroids = np.zeros_like(centroids)\n",
    "    for cluster in range(num_cluster):\n",
    "        indices = np.where(cluster_assignments == cluster)[0]  # Indices of points in this cluster\n",
    "        if len(indices) > 0:\n",
    "            cluster_label = mode(y[indices]).mode[0]  # Most common label in the cluster\n",
    "            if cluster_label < num_cluster:  # Ensure the label is within the range of cluster numbers\n",
    "                reordered_centroids[cluster_label] = centroids[cluster]\n",
    "    \n",
    "    return reordered_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f484d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # Create a numpy array filled with zeros and of appropriate size\n",
    "    one_hot = np.zeros((labels.shape[0], num_classes))\n",
    "    # Use fancy indexing to place ones where the class label indicates\n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_mutual_information(p_1, p_2, p_12, eps=1e-10):\n",
    "    # Add epsilon to avoid taking log(0)\n",
    "    p_12 = p_12 + eps\n",
    "\n",
    "    p1_p2 = np.outer(p_1, p_2)  # A x B\n",
    "    p1_p2 = p1_p2 + eps  # Add epsilon to the denominator to prevent division by zero\n",
    "\n",
    "    mi = np.sum(p_12 * np.log(p_12 / p1_p2))\n",
    "    return mi\n",
    "\n",
    "def get_clust_score(feat, centroids, beta=1.0):\n",
    "    # compute cluster score for each feature\n",
    "    # feat: N x D\n",
    "    # centroids: K x D\n",
    "    # return: N x K\n",
    "\n",
    "    N = feat.shape[0]\n",
    "    K = centroids.shape[0]\n",
    "    score = np.zeros((N, K))\n",
    "    feat_normalized = feat / np.linalg.norm(feat, axis=1, keepdims=True)  # Normalize features\n",
    "    \n",
    "    #这里的centroids 也应该normalize一下？\n",
    "    \n",
    "    for i in range(K):\n",
    "        score[:, i] = np.linalg.norm(feat_normalized - centroids[i], axis=1) ** 2\n",
    "    score = -beta * score\n",
    "    score = np.exp(score)\n",
    "    score /= np.sum(score, axis=1, keepdims=True)  # softmax\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287b2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero,\n",
    "                 dropout=0.,\n",
    "                 is_sparse_inputs=False,\n",
    "                 bias=False,\n",
    "                 activation=True,\n",
    "                 featureless=False):\n",
    "        super(Linear, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.activation = activation\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training and self.is_sparse_inputs:\n",
    "            x = sparse_dropout(x, self.dropout, self.num_features_nonzero)\n",
    "        elif self.training:\n",
    "            x = F.dropout(x, self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        if not self.featureless: # if it has features x\n",
    "            if self.is_sparse_inputs:\n",
    "                xw = torch.sparse.mm(x, self.weight)\n",
    "            else:\n",
    "                xw = torch.mm(x, self.weight)\n",
    "        else:\n",
    "            xw = self.weight\n",
    "\n",
    "        out = xw\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        if self.activation:\n",
    "            out = F.relu(out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero,\n",
    "                 dropout=0.,\n",
    "                 is_sparse_inputs=False,\n",
    "                 bias=False,\n",
    "                 activation=True,\n",
    "                 featureless=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.activation = activation\n",
    "        self.is_sparse_inputs = is_sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x, support):\n",
    "\n",
    "        if self.training and self.is_sparse_inputs:\n",
    "            x = sparse_dropout(x, self.dropout, self.num_features_nonzero)\n",
    "        elif self.training:\n",
    "            x = F.dropout(x, self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        if not self.featureless: # if it has features x\n",
    "            if self.is_sparse_inputs:\n",
    "                xw = torch.sparse.mm(x, self.weight)\n",
    "            else:\n",
    "                xw = torch.mm(x, self.weight)\n",
    "        else:\n",
    "            xw = self.weight\n",
    "\n",
    "        out = torch.sparse.mm(support, xw)\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        if self.activation:\n",
    "            out = F.relu(out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "class GCN_base(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, args):\n",
    "        super(GCN_base, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim # 1433\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        print('input dim:', input_dim)\n",
    "        print('output dim:', output_dim)\n",
    "        print('num_features_nonzero:', num_features_nonzero)\n",
    "   \n",
    "        self.gcn_1 = GraphConvolution(self.input_dim, args.hidden, num_features_nonzero,\n",
    "                                    activation=True,\n",
    "                                    dropout=args.dropout,\n",
    "                                    is_sparse_inputs=True)\n",
    "\n",
    "        self.gcn_2 = GraphConvolution(args.hidden, output_dim, num_features_nonzero,\n",
    "                                    activation=True,\n",
    "                                    dropout=args.dropout,\n",
    "                                    is_sparse_inputs=False)   \n",
    "        \n",
    "\n",
    "    def forward(self, x, support):\n",
    "\n",
    "        out = self.gcn_1(x, support)\n",
    "        out = self.gcn_2(out, support)\n",
    "        return out\n",
    "    \n",
    "    def l2_loss(self):\n",
    "        loss = None\n",
    "        for p in self.gcn_1.parameters():\n",
    "            if loss is None:\n",
    "                loss = p.pow(2).sum()\n",
    "            else:\n",
    "                loss += p.pow(2).sum()\n",
    "\n",
    "        for p in self.gcn_2.parameters():\n",
    "            if loss is None:\n",
    "                loss = p.pow(2).sum()\n",
    "            else:\n",
    "                loss += p.pow(2).sum()\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12064e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj: (2708, 2708)\n",
      "features: (2708, 1433)\n",
      "y: (2708, 7) (2708, 7) (2708, 7)\n",
      "mask: (2708,) (2708,) (2708,)\n"
     ]
    }
   ],
   "source": [
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, y_all = load_data_all(args.dataset)\n",
    "print('adj:', adj.shape)\n",
    "print('features:', features.shape)\n",
    "print('y:', y_train.shape, y_val.shape, y_test.shape)\n",
    "print('mask:', train_mask.shape, val_mask.shape, test_mask.shape)\n",
    "\n",
    "# D^-1@X\n",
    "features = preprocess_features(features) # [49216, 2], [49216], [2708, 1433]\n",
    "supports = preprocess_adj(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fba2cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "train_label = torch.from_numpy(y_train).long().to(device)\n",
    "num_classes = train_label.shape[1]\n",
    "train_label = train_label.argmax(dim=1)\n",
    "train_mask = torch.from_numpy(train_mask.astype(np.int64)).to(device)\n",
    "val_label = torch.from_numpy(y_val).long().to(device)\n",
    "val_label = val_label.argmax(dim=1)\n",
    "val_mask = torch.from_numpy(val_mask.astype(np.int64)).to(device)\n",
    "test_label = torch.from_numpy(y_test).long().to(device)\n",
    "test_label = test_label.argmax(dim=1)\n",
    "test_mask = torch.from_numpy(test_mask.astype(np.int64)).to(device)\n",
    "\n",
    "i = torch.from_numpy(features[0]).long().to(device)\n",
    "v = torch.from_numpy(features[1]).to(device)\n",
    "feature = torch.sparse.FloatTensor(i.t(), v, features[2]).to(device)\n",
    "\n",
    "i = torch.from_numpy(supports[0]).long().to(device)\n",
    "v = torch.from_numpy(supports[1]).to(device)\n",
    "support = torch.sparse.FloatTensor(i.t(), v, supports[2]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f8397b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "                       [1274, 1247, 1194,  ...,  329,  186,   19]]),\n",
       "       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),\n",
       "       size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ea92010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj: (2708, 2708)\n",
      "features: (2708, 1433)\n",
      "y: (2708, 7) (2708, 7) (2708, 7)\n",
      "mask: (2708,) (2708,) (2708,)\n",
      "x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "                       [1274, 1247, 1194,  ...,  329,  186,   19]]),\n",
      "       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),\n",
      "       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)\n",
      "sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],\n",
      "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
      "       values=tensor([0.2500, 0.2500, 0.2236,  ..., 0.2000, 0.2000, 0.2000]),\n",
      "       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)\n",
      "input dim: 1433\n",
      "output dim: 7\n",
      "num_features_nonzero: 49216\n",
      "Epoch: 10 train loss: 1.691896 test loss: 1.864222 train acc: 0.5071 test acc: 0.2650\n",
      "Epoch: 20 train loss: 1.334045 test loss: 1.701272 train acc: 0.6500 test acc: 0.4210\n",
      "Epoch: 30 train loss: 1.014300 test loss: 1.466798 train acc: 0.7643 test acc: 0.5160\n",
      "Epoch: 40 train loss: 0.773818 test loss: 1.238416 train acc: 0.8286 test acc: 0.6620\n",
      "Epoch: 50 train loss: 0.624847 test loss: 1.105864 train acc: 0.9214 test acc: 0.7130\n",
      "Epoch: 60 train loss: 0.468868 test loss: 0.970898 train acc: 0.9857 test acc: 0.7940\n",
      "Epoch: 70 train loss: 0.320601 test loss: 0.844660 train acc: 0.9929 test acc: 0.8090\n",
      "Epoch: 80 train loss: 0.275867 test loss: 0.805988 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 90 train loss: 0.265644 test loss: 0.800164 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 100 train loss: 0.260429 test loss: 0.796645 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 110 train loss: 0.259200 test loss: 0.796095 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 120 train loss: 0.258395 test loss: 0.795927 train acc: 1.0000 test acc: 0.8060\n",
      "Epoch: 130 train loss: 0.257561 test loss: 0.795590 train acc: 1.0000 test acc: 0.8050\n",
      "Epoch: 140 train loss: 0.256885 test loss: 0.795080 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 150 train loss: 0.256201 test loss: 0.794830 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 160 train loss: 0.255644 test loss: 0.794676 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 170 train loss: 0.255152 test loss: 0.794617 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 180 train loss: 0.254788 test loss: 0.794572 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 190 train loss: 0.254405 test loss: 0.794525 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 200 train loss: 0.254095 test loss: 0.794110 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 210 train loss: 0.253779 test loss: 0.794100 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 220 train loss: 0.253506 test loss: 0.794121 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 230 train loss: 0.253271 test loss: 0.794085 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 240 train loss: 0.253059 test loss: 0.794051 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 250 train loss: 0.252881 test loss: 0.794110 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 260 train loss: 0.252690 test loss: 0.793654 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 270 train loss: 0.252546 test loss: 0.793680 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 280 train loss: 0.252407 test loss: 0.793809 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 290 train loss: 0.252231 test loss: 0.793613 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 300 train loss: 0.252172 test loss: 0.793441 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 310 train loss: 0.252057 test loss: 0.793402 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 320 train loss: 0.251928 test loss: 0.793263 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 330 train loss: 0.251864 test loss: 0.793403 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 340 train loss: 0.251728 test loss: 0.793330 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 350 train loss: 0.251682 test loss: 0.793120 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 360 train loss: 0.251623 test loss: 0.793114 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 370 train loss: 0.251504 test loss: 0.792996 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 380 train loss: 0.251443 test loss: 0.793004 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 390 train loss: 0.251354 test loss: 0.792993 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 400 train loss: 0.251305 test loss: 0.793264 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 410 train loss: 0.251274 test loss: 0.792925 train acc: 1.0000 test acc: 0.8140\n",
      "Epoch: 420 train loss: 0.251202 test loss: 0.792854 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 430 train loss: 0.251153 test loss: 0.792646 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 440 train loss: 0.251111 test loss: 0.792593 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 450 train loss: 0.251132 test loss: 0.792765 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 460 train loss: 0.251060 test loss: 0.792544 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 470 train loss: 0.251002 test loss: 0.792572 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 480 train loss: 0.250998 test loss: 0.792670 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 490 train loss: 0.250951 test loss: 0.792145 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 500 train loss: 0.250929 test loss: 0.792335 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 510 train loss: 0.250875 test loss: 0.792053 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 520 train loss: 0.250827 test loss: 0.792168 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 530 train loss: 0.250825 test loss: 0.792483 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 540 train loss: 0.250774 test loss: 0.792126 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 550 train loss: 0.250754 test loss: 0.792263 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 560 train loss: 0.250714 test loss: 0.792066 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 570 train loss: 0.250706 test loss: 0.792318 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 580 train loss: 0.250630 test loss: 0.791782 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 590 train loss: 0.250600 test loss: 0.792259 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 600 train loss: 0.250577 test loss: 0.792018 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 610 train loss: 0.250589 test loss: 0.791846 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 620 train loss: 0.250502 test loss: 0.791966 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 630 train loss: 0.250513 test loss: 0.792328 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 640 train loss: 0.250493 test loss: 0.791892 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 650 train loss: 0.250460 test loss: 0.792045 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 660 train loss: 0.250347 test loss: 0.791573 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 670 train loss: 0.250339 test loss: 0.792015 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 680 train loss: 0.250297 test loss: 0.791877 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 690 train loss: 0.250277 test loss: 0.791631 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 700 train loss: 0.250251 test loss: 0.791711 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 710 train loss: 0.250237 test loss: 0.792038 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 720 train loss: 0.250191 test loss: 0.792142 train acc: 1.0000 test acc: 0.8130\n",
      "Epoch: 730 train loss: 0.250133 test loss: 0.791808 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 740 train loss: 0.250130 test loss: 0.791465 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 750 train loss: 0.250106 test loss: 0.791934 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 760 train loss: 0.250038 test loss: 0.791655 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 770 train loss: 0.249988 test loss: 0.791888 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 780 train loss: 0.249966 test loss: 0.791713 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 790 train loss: 0.249935 test loss: 0.792211 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 800 train loss: 0.249899 test loss: 0.792016 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 810 train loss: 0.249889 test loss: 0.792522 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 820 train loss: 0.249841 test loss: 0.792094 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 830 train loss: 0.249818 test loss: 0.792004 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 840 train loss: 0.249785 test loss: 0.792129 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 850 train loss: 0.249773 test loss: 0.792535 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 860 train loss: 0.249758 test loss: 0.791915 train acc: 1.0000 test acc: 0.8120\n",
      "Epoch: 870 train loss: 0.249741 test loss: 0.792301 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 880 train loss: 0.249684 test loss: 0.792276 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 890 train loss: 0.249671 test loss: 0.791904 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 900 train loss: 0.249614 test loss: 0.791813 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 910 train loss: 0.249584 test loss: 0.792016 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 920 train loss: 0.249567 test loss: 0.791679 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 930 train loss: 0.249551 test loss: 0.791962 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 940 train loss: 0.249577 test loss: 0.791901 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 950 train loss: 0.249496 test loss: 0.792339 train acc: 1.0000 test acc: 0.8110\n",
      "Epoch: 960 train loss: 0.249511 test loss: 0.791554 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 970 train loss: 0.249539 test loss: 0.791298 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 980 train loss: 0.249429 test loss: 0.791444 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 990 train loss: 0.249391 test loss: 0.791391 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1000 train loss: 0.249435 test loss: 0.791706 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1010 train loss: 0.249383 test loss: 0.792137 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1020 train loss: 0.249456 test loss: 0.792362 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1030 train loss: 0.249350 test loss: 0.791641 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1040 train loss: 0.249412 test loss: 0.791454 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1050 train loss: 0.249325 test loss: 0.791487 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1060 train loss: 0.249274 test loss: 0.792264 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1070 train loss: 0.249320 test loss: 0.791366 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1080 train loss: 0.249292 test loss: 0.792418 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1090 train loss: 0.249228 test loss: 0.791869 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1100 train loss: 0.249250 test loss: 0.792097 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1110 train loss: 0.249231 test loss: 0.790887 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1120 train loss: 0.249209 test loss: 0.791885 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1130 train loss: 0.249264 test loss: 0.792279 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1140 train loss: 0.249193 test loss: 0.791773 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1150 train loss: 0.249195 test loss: 0.791450 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1160 train loss: 0.249126 test loss: 0.792399 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1170 train loss: 0.249176 test loss: 0.791428 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1180 train loss: 0.249169 test loss: 0.791753 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1190 train loss: 0.249147 test loss: 0.792305 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1200 train loss: 0.249147 test loss: 0.791634 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1210 train loss: 0.249141 test loss: 0.791588 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1220 train loss: 0.249123 test loss: 0.790811 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1230 train loss: 0.249045 test loss: 0.791847 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1240 train loss: 0.249063 test loss: 0.792145 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1250 train loss: 0.249061 test loss: 0.791572 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1260 train loss: 0.249128 test loss: 0.791233 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1270 train loss: 0.249099 test loss: 0.791515 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1280 train loss: 0.249002 test loss: 0.791619 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1290 train loss: 0.249052 test loss: 0.791483 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1300 train loss: 0.249066 test loss: 0.792130 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1310 train loss: 0.249021 test loss: 0.792023 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1320 train loss: 0.248994 test loss: 0.792198 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1330 train loss: 0.248981 test loss: 0.792202 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1340 train loss: 0.248946 test loss: 0.791987 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1350 train loss: 0.248976 test loss: 0.792434 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1360 train loss: 0.248937 test loss: 0.792014 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1370 train loss: 0.248968 test loss: 0.792090 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1380 train loss: 0.248983 test loss: 0.792068 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1390 train loss: 0.248963 test loss: 0.791285 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1400 train loss: 0.248946 test loss: 0.791644 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1410 train loss: 0.248961 test loss: 0.791853 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1420 train loss: 0.248884 test loss: 0.791047 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1430 train loss: 0.248860 test loss: 0.792781 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1440 train loss: 0.248929 test loss: 0.791873 train acc: 1.0000 test acc: 0.8100\n",
      "Epoch: 1450 train loss: 0.248918 test loss: 0.792354 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1460 train loss: 0.248900 test loss: 0.792584 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1470 train loss: 0.249006 test loss: 0.791472 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1480 train loss: 0.248965 test loss: 0.792705 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1490 train loss: 0.248897 test loss: 0.791267 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1500 train loss: 0.248938 test loss: 0.791929 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1510 train loss: 0.248903 test loss: 0.792189 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1520 train loss: 0.248980 test loss: 0.793162 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1530 train loss: 0.248896 test loss: 0.791028 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1540 train loss: 0.248892 test loss: 0.790441 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1550 train loss: 0.248884 test loss: 0.791039 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1560 train loss: 0.248830 test loss: 0.791961 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1570 train loss: 0.248890 test loss: 0.791782 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1580 train loss: 0.248893 test loss: 0.791433 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1590 train loss: 0.248836 test loss: 0.792281 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1600 train loss: 0.248777 test loss: 0.790925 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1610 train loss: 0.248846 test loss: 0.791900 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1620 train loss: 0.248835 test loss: 0.791320 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1630 train loss: 0.248832 test loss: 0.792825 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1640 train loss: 0.248945 test loss: 0.792086 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1650 train loss: 0.248846 test loss: 0.793292 train acc: 1.0000 test acc: 0.8050\n",
      "Epoch: 1660 train loss: 0.248860 test loss: 0.791454 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1670 train loss: 0.248848 test loss: 0.791673 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1680 train loss: 0.248919 test loss: 0.792674 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1690 train loss: 0.248798 test loss: 0.792529 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1700 train loss: 0.248878 test loss: 0.791757 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1710 train loss: 0.248875 test loss: 0.792154 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1720 train loss: 0.248822 test loss: 0.791181 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 1730 train loss: 0.248837 test loss: 0.792680 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1740 train loss: 0.248975 test loss: 0.790814 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1750 train loss: 0.248904 test loss: 0.792490 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1760 train loss: 0.248862 test loss: 0.792267 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1770 train loss: 0.248846 test loss: 0.792111 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1780 train loss: 0.248866 test loss: 0.791750 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1790 train loss: 0.248884 test loss: 0.791536 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1800 train loss: 0.248792 test loss: 0.790988 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 1810 train loss: 0.248908 test loss: 0.792711 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1820 train loss: 0.248883 test loss: 0.791675 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1830 train loss: 0.248867 test loss: 0.792316 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1840 train loss: 0.248828 test loss: 0.791937 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 1850 train loss: 0.248938 test loss: 0.792610 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1860 train loss: 0.248847 test loss: 0.793104 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1870 train loss: 0.248806 test loss: 0.792167 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1880 train loss: 0.248808 test loss: 0.791319 train acc: 1.0000 test acc: 0.8070\n",
      "Epoch: 1890 train loss: 0.248854 test loss: 0.792537 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1900 train loss: 0.248864 test loss: 0.792444 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1910 train loss: 0.248794 test loss: 0.791752 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1920 train loss: 0.248901 test loss: 0.791682 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1930 train loss: 0.248822 test loss: 0.790905 train acc: 1.0000 test acc: 0.8080\n",
      "Epoch: 1940 train loss: 0.248855 test loss: 0.792335 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1950 train loss: 0.248741 test loss: 0.791842 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1960 train loss: 0.248946 test loss: 0.793217 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1970 train loss: 0.248856 test loss: 0.791304 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1980 train loss: 0.248810 test loss: 0.791647 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 1990 train loss: 0.248814 test loss: 0.792822 train acc: 1.0000 test acc: 0.8090\n",
      "Epoch: 2000 train loss: 0.248806 test loss: 0.792103 train acc: 1.0000 test acc: 0.8090\n",
      "best test acc: 0.8149998784065247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, y_all = load_data_all(args.dataset)\n",
    "print('adj:', adj.shape)\n",
    "print('features:', features.shape)\n",
    "print('y:', y_train.shape, y_val.shape, y_test.shape)\n",
    "print('mask:', train_mask.shape, val_mask.shape, test_mask.shape)\n",
    "\n",
    "# D^-1@X\n",
    "features = preprocess_features(features) # [49216, 2], [49216], [2708, 1433]\n",
    "supports = preprocess_adj(adj)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "train_label = torch.from_numpy(y_train).long().to(device)\n",
    "num_classes = train_label.shape[1]\n",
    "train_label = train_label.argmax(dim=1)\n",
    "train_mask = torch.from_numpy(train_mask.astype(np.int64)).to(device)\n",
    "val_label = torch.from_numpy(y_val).long().to(device)\n",
    "val_label = val_label.argmax(dim=1)\n",
    "val_mask = torch.from_numpy(val_mask.astype(np.int64)).to(device)\n",
    "test_label = torch.from_numpy(y_test).long().to(device)\n",
    "test_label = test_label.argmax(dim=1)\n",
    "test_mask = torch.from_numpy(test_mask.astype(np.int64)).to(device)\n",
    "\n",
    "i = torch.from_numpy(features[0]).long().to(device)\n",
    "v = torch.from_numpy(features[1]).to(device)\n",
    "feature = torch.sparse.FloatTensor(i.t(), v, features[2]).to(device)\n",
    "\n",
    "i = torch.from_numpy(supports[0]).long().to(device)\n",
    "v = torch.from_numpy(supports[1]).to(device)\n",
    "support = torch.sparse.FloatTensor(i.t(), v, supports[2]).float().to(device)\n",
    "\n",
    "print('x :', feature)\n",
    "print('sp:', support)\n",
    "num_features_nonzero = feature._nnz()\n",
    "feat_dim = feature.shape[1]\n",
    "\n",
    "net = GCN_base(feat_dim, num_classes, num_features_nonzero, args)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "net.train()\n",
    "acc_test_history=[]\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "acc_hist = []\n",
    "for epoch in range(args.epochs):\n",
    "    net.train()\n",
    "    out = net(feature, support)\n",
    "    \n",
    "    loss = masked_loss(out, train_label, train_mask)\n",
    "    #这个loss为什么要+后面l2loss？\n",
    "    loss_all = loss + args.weight_decay * net.l2_loss()\n",
    "    # loss_all = loss\n",
    "    optimizer.zero_grad()\n",
    "    loss_all.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    out = net(feature, support)\n",
    "    test_loss = masked_loss(out, test_label, test_mask)\n",
    "    acc_train = masked_acc(out, train_label, train_mask)\n",
    "    acc_test = masked_acc(out, test_label, test_mask)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch:', epoch+1, 'train loss:', '%.6f' % loss.item(), 'test loss:', '%.6f' % test_loss.item(), 'train acc:', '%.4f' % acc_train.item(), 'test acc:', '%.4f' % acc_test.item())\n",
    "    train_loss_hist.append(loss.item())\n",
    "    test_loss_hist.append(test_loss.item())\n",
    "    acc_hist.append(acc_test.item())\n",
    "    acc_test_history.append(acc_test.item())\n",
    "    acc_test_all = np.array(acc_test_history)\n",
    "    if np.max(acc_test_all) > best_acc:\n",
    "        best_acc = np.max(acc_test_all)        \n",
    "        np.save('gcn_out_'+args.dataset+'.npy', out.cpu().data.numpy())\n",
    "net.eval()\n",
    "out = net(feature, support)\n",
    "# out = out[0]\n",
    "acc = masked_acc(out, test_label, test_mask)\n",
    "acc_test_all = np.array(acc_test_history)\n",
    "print('best test acc:', best_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12d6476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat: (2708, 7)\n",
      "input feat: (2708, 1433)\n",
      "performing kmeans clustering\n",
      "performing kmeans clustering\n",
      "The MI_in_out is:  2.243115284471518e-05\n",
      "The MI_out_label is:  0.4326593000668484\n",
      "The information bottleneck is:  -0.43263686891400366\n"
     ]
    }
   ],
   "source": [
    "inter_feat = net(feature, support).cpu().data.numpy()\n",
    "print('feat:', inter_feat.shape)\n",
    "\n",
    "input_feat = feature.to_dense().cpu().data.numpy()\n",
    "print('input feat:', input_feat.shape)\n",
    "\n",
    "y_all_1d = y_all.argmax(axis=1)\n",
    "\n",
    "inter_centroids = run_kmeans(inter_feat, y_all_1d, num_classes)\n",
    "input_centroids = run_kmeans(input_feat, y_all_1d, num_classes)\n",
    "\n",
    "inter_score = get_clust_score(inter_feat, inter_centroids)\n",
    "input_score = get_clust_score(input_feat, input_centroids)\n",
    "\n",
    "# inter_label = np.argmax(inter_score, axis=1)\n",
    "# input_label = np.argmax(input_score, axis=1)\n",
    "# inter_acc = np.sum(inter_label == y_all_1d) / len(y_all_1d)\n",
    "# input_acc = np.sum(input_label == y_all_1d) / len(y_all_1d)\n",
    "# print('inter acc:', inter_acc)\n",
    "# print('input acc:', input_acc)\n",
    "\n",
    "clust_score_input =  get_clust_score(input_feat, input_centroids)\n",
    "p_in = np.sum(clust_score_input, axis=0)\n",
    "\n",
    "clust_score_output = get_clust_score(inter_feat, inter_centroids)\n",
    "p_out = np.sum(clust_score_output, axis=0)\n",
    "\n",
    "one_hot_target = y_all\n",
    "p_label = np.sum(one_hot_target, axis=0)\n",
    "\n",
    "p_in_out = np.sum(np.matmul(clust_score_input[:, :, np.newaxis], \n",
    "                            clust_score_output[:, np.newaxis, :]), axis=0)\n",
    "\n",
    "p_out_label = np.sum(np.matmul(clust_score_output[:, :, np.newaxis], \n",
    "                               one_hot_target[:, np.newaxis, :]), axis=0)\n",
    "\n",
    "p_in = p_in / np.sum(p_in)\n",
    "p_out = p_out / np.sum(p_out)\n",
    "p_label = p_label / np.sum(p_label)\n",
    "p_in_out = p_in_out / np.sum(p_in_out)\n",
    "p_out_label = p_out_label / np.sum(p_out_label)\n",
    "\n",
    "MI_in_out = get_mutual_information(p_in, p_out, p_in_out)\n",
    "MI_out_label = get_mutual_information(p_out, p_label, p_out_label)\n",
    "information_bottleneck = MI_in_out - MI_out_label\n",
    "print('The MI_in_out is: ', MI_in_out.item())\n",
    "print('The MI_out_label is: ', MI_out_label.item())\n",
    "print('The information bottleneck is: ', information_bottleneck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87fbb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat: (2708, 32)\n",
      "input feat: (2708, 1433)\n",
      "performing kmeans clustering\n",
      "performing kmeans clustering\n",
      "The MI_in_out is:  8.699037547742988e-07\n",
      "The MI_out_label is:  0.009720624638259708\n",
      "The information bottleneck is:  -0.009719754734504933\n"
     ]
    }
   ],
   "source": [
    "inter_feat = net.gcn_1(feature, support).cpu().data.numpy()\n",
    "print('feat:', inter_feat.shape)\n",
    "\n",
    "input_feat = feature.to_dense().cpu().data.numpy()\n",
    "print('input feat:', input_feat.shape)\n",
    "\n",
    "y_all_1d = y_all.argmax(axis=1)\n",
    "\n",
    "inter_centroids = run_kmeans(inter_feat, y_all_1d, num_classes)\n",
    "input_centroids = run_kmeans(input_feat, y_all_1d, num_classes)\n",
    "\n",
    "inter_score = get_clust_score(inter_feat, inter_centroids)\n",
    "input_score = get_clust_score(input_feat, input_centroids)\n",
    "\n",
    "# inter_label = np.argmax(inter_score, axis=1)\n",
    "# input_label = np.argmax(input_score, axis=1)\n",
    "# inter_acc = np.sum(inter_label == y_all_1d) / len(y_all_1d)\n",
    "# input_acc = np.sum(input_label == y_all_1d) / len(y_all_1d)\n",
    "# print('inter acc:', inter_acc)\n",
    "# print('input acc:', input_acc)\n",
    "\n",
    "clust_score_input =  get_clust_score(input_feat, input_centroids)\n",
    "p_in = np.sum(clust_score_input, axis=0)\n",
    "\n",
    "clust_score_output = get_clust_score(inter_feat, inter_centroids)\n",
    "p_out = np.sum(clust_score_output, axis=0)\n",
    "\n",
    "one_hot_target = y_all\n",
    "p_label = np.sum(one_hot_target, axis=0)\n",
    "\n",
    "p_in_out = np.sum(np.matmul(clust_score_input[:, :, np.newaxis], \n",
    "                            clust_score_output[:, np.newaxis, :]), axis=0)\n",
    "\n",
    "p_out_label = np.sum(np.matmul(clust_score_output[:, :, np.newaxis], \n",
    "                               one_hot_target[:, np.newaxis, :]), axis=0)\n",
    "\n",
    "p_in = p_in / np.sum(p_in)\n",
    "p_out = p_out / np.sum(p_out)\n",
    "p_label = p_label / np.sum(p_label)\n",
    "p_in_out = p_in_out / np.sum(p_in_out)\n",
    "p_out_label = p_out_label / np.sum(p_out_label)\n",
    "\n",
    "MI_in_out = get_mutual_information(p_in, p_out, p_in_out)\n",
    "MI_out_label = get_mutual_information(p_out, p_label, p_out_label)\n",
    "information_bottleneck = MI_in_out - MI_out_label\n",
    "print('The MI_in_out is: ', MI_in_out.item())\n",
    "print('The MI_out_label is: ', MI_out_label.item())\n",
    "print('The information bottleneck is: ', information_bottleneck)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
